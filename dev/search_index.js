var documenterSearchIndex = {"docs":
[{"location":"#","page":"Home","title":"Home","text":"(Image: AugmentedGaussianProcesses.jl)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"(Image: Docs Latest) (Image: Docs Stable) (Image: Build Status) (Image: Coverage Status)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A Julia package for Augmented and Normal Gaussian Processes.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Authors-1","page":"Home","title":"Authors","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"ThÃ©o Galy-Fajou PhD Student at Technical University of Berlin.\nFlorian Wenzel PhD Student at Technical University of Kaiserslautern & Humboldt University of Berlin","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"AugmentedGaussianProcesses is a registered package and is symply installed by running","category":"page"},{"location":"#","page":"Home","title":"Home","text":"pkg> add AugmentedGaussianProcesses","category":"page"},{"location":"#Basic-example-1","page":"Home","title":"Basic example","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Here is a simple example to start right away :","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using AugmentedGaussianProcesses\nmodel = SVGP(X_train,y_train,RBFKernel(1.0),LogisticLikelihood(),AnalyticVI(),50)\ntrain!(model,iterations=100)\ny_pred = predict_y(model,X_test)","category":"page"},{"location":"#Related-Gaussian-Processes-packages-1","page":"Home","title":"Related Gaussian Processes packages","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"GaussianProcesses.jl : General package for Gaussian Processes with many available likelihoods\nStheno.jl : Package for Gaussian Process regression","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A general comparison between this package is done on Julia GP Package Comparison. Benchmark evaluations may come later.","category":"page"},{"location":"#License-1","page":"Home","title":"License","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"AugmentedGaussianProcesses.jl is licensed under the MIT \"Expat\" license; see LICENSE for the full license text.","category":"page"},{"location":"background/#The-bits-of-math-and-science-behind-it-1","page":"Background","title":"The bits of math and science behind it","text":"","category":"section"},{"location":"background/#Gaussian-Processes-1","page":"Background","title":"Gaussian Processes","text":"","category":"section"},{"location":"background/#","page":"Background","title":"Background","text":"To quote Wikipedia \"A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\"","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"For a detailed understanding of Gaussian processes, check the wonderful book of Rasmussen and Williams and for a quick introduction, check this tutorial by Zoubin Ghahramani","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Gaussian Processes are extremely practical model since they are non-parametric and are Bayesian. However the basic model is limited to regression with Gaussian noise and does not scale very well to large datasets (>1000 samples). The Augmented Gaussian Processes solve both these problems by adding inducing points as well as transforming the likelihood to get efficient variational inference.","category":"page"},{"location":"background/#Augmented-Gaussian-Processes-1","page":"Background","title":"Augmented Gaussian Processes","text":"","category":"section"},{"location":"background/#","page":"Background","title":"Background","text":"We are interested in models which consist of a GP prior on a latent function fsim textGP(0k), where k is the kernel function and the data y is connected to f via a non-conjugate likelihood p(yf) . We now aim on finding an augmented representation of the model which renders the model conditionally conjugate. Let omega be potential augmentation, then the augmented joint distribution is","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"p(yfomega) =p(yfomega)p(omega)p(f)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The original model can be restored by marginalizing omega, i.e. p(yf) =int p(yfomega)domega.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The  goal  is  to  find  an  augmentation omega,  such  that  the  augmented  likelihood p(yfomega) becomes conjugate to the prior distributions p(f) and p(omega) and the expectations of the log complete conditional distributions log p(fomegay) and log p(omegafy) can be computed in closed-form.","category":"page"},{"location":"background/#How-to-find-a-suitable-augmentation?-1","page":"Background","title":"How to find a suitable augmentation?","text":"","category":"section"},{"location":"background/#","page":"Background","title":"Background","text":"Many popular likelihood functions can be expressed as ascale mixture of Gaussians","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"p(yf) =int N(yBftextdiag(omega^1))p(omega)domega","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"where B is a matrix (Palmer et al., 2006).  This representation directly leads to the augmented likelihood p(yomegaf) =N(yBftextdiag(omega^1)) which is conjugate in f, i.e. the posterior is again a Gaussian. I am currently working on a generalized  and automatic approach, which should be available during this year.","category":"page"},{"location":"background/#Inference-in-the-augmented-model-1","page":"Background","title":"Inference in the augmented model","text":"","category":"section"},{"location":"background/#","page":"Background","title":"Background","text":"If we assume that the augmentation, discussed in the previous section, was successful and that we obtained an augmented model p(yfomega) = p(yfomega)p(f)p(omega) which is conditionally conjugate. In a conditionally conjugate model variational inference is easy and block coordinate ascent updates can be computed in closed-form. We follow as structured mean-field approach and assume a decoupling between the latent GP f and the auxiliary variable omega in the variational distribution q(fomega) = q(f) q(omega).  We alternate between updating q(f) and q(omega) by using the typical coordinate ascent (CAVI) updates building on expectations of the log complete conditionals.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The hyperparameter of the latent GP (e.g. length scale) are learned by optimizing the variational lower bound as function of the hyper parameters. We alternate between updating the variational parameters and the hyperparameters.","category":"page"},{"location":"background/#Sparse-Gaussian-Processes-1","page":"Background","title":"Sparse Gaussian Processes","text":"","category":"section"},{"location":"background/#","page":"Background","title":"Background","text":"Direct inference for GPs has a cubic computational complexity mathcalO(N^3). To scale our model to big datasets we approximate the latent GP by a sparse GP building on inducing points. This reduces the complexity to mathcalO(M^3), where M is the number of inducing points. Using inducing points allows us to employ stochastic variational inference (SVI) that computes the updates based on mini-batches of the data.","category":"page"},{"location":"userguide/#User-Guide-1","page":"User Guide","title":"User Guide","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"There are 3 main actions needed to train and use the different models:","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"Initialization\nTraining\nPrediction","category":"page"},{"location":"userguide/#init-1","page":"User Guide","title":"Initialization","text":"","category":"section"},{"location":"userguide/#GP-vs-VGP-vs-SVGP-1","page":"User Guide","title":"GP vs VGP vs SVGP","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"There are currently 3 possible Gaussian Process models:","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"GP corresponds to the original GP regression model, it is necessarily with a Gaussian likelihood.","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"    GP(X_train,y_train,kernel)","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"VGP is a variational GP model: a multivariate Gaussian is approximating the true posterior. There is no inducing points augmentation involved. Therefore it is well suited for small datasets (~10^3 samples)","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"    VGP(X_train,y_train,kernel,likelihood,inference)","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"SVGP is a variational GP model augmented with inducing points. The optimization is done on those points, allowing for stochastic updates and large scalability. The counterpart can be a slightly lower accuracy and the need to select the number and the location of the inducing points (however this is a problem currently worked on).","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"    SVGP(X_train,y_train,kernel,likelihood,inference,n_inducingpoints)","category":"page"},{"location":"userguide/#likelihood_user-1","page":"User Guide","title":"Likelihood","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"GP can only have a Gaussian likelihood, VGP and SVGP have more choices. Here are the ones currently implemented:","category":"page"},{"location":"userguide/#Regression-1","page":"User Guide","title":"Regression","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"For regression, four likelihoods are available :","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"The classical GaussianLikelihood, for Gaussian noise\nThe StudentTLikelihood, assuming noise from a Student-T distribution (more robust to ouliers)\nThe LaplaceLikelihood, with noise from a Laplace distribution.\nThe HeteroscedasticLikelihood, (in development) where the noise is a function of the input: textVar(X) = lambdasigma^-1(g(X)) where g(X) is an additional Gaussian Process and sigma is the logistic function.","category":"page"},{"location":"userguide/#Classification-1","page":"User Guide","title":"Classification","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"For classification one can select among","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"The LogisticLikelihood : a Bernoulli likelihood with a logistic link\nThe BayesianSVM likelihood based on the frequentist SVM, equivalent to use a hinge loss.","category":"page"},{"location":"userguide/#Event-Likelihoods-1","page":"User Guide","title":"Event Likelihoods","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"For likelihoods such as Poisson or Negative Binomial, we approximate a parameter by Ï(f). Two Likelihoods are implemented :","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"The PoissonLikelihood : A discrete Poisson process (one parameter per point) with the scale parameter defined as Î»Ï(f)\nThe NegBinomialLikelihood : The Negative Binomial likelihood where r is fixed and we define the success probability p as Ï(f)","category":"page"},{"location":"userguide/#Multi-class-classification-1","page":"User Guide","title":"Multi-class classification","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"There is two available likelihoods for multi-class classification:","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"The SoftMaxLikelihood, the most common approach. However no analytical solving is possible\nThe LogisticSoftMaxLikelihood, a modified softmax where the exponential function is replaced by the logistic function. It allows to get a fully conjugate model, Corresponding paper","category":"page"},{"location":"userguide/#More-options-1","page":"User Guide","title":"More options","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"You can also write your own likelihood by using the following template.","category":"page"},{"location":"userguide/#Inference-1","page":"User Guide","title":"Inference","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"Inference can be done in various ways.","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"AnalyticVI : Variational Inference with closed-form updates. For non-Gaussian likelihoods, this relies on augmented version of the likelihoods. For using Stochastic Variational Inference, one can use AnalyticSVI with the size of the mini-batch as an argument\nGibbsSampling : Gibbs Sampling of the true posterior, this also rely on an augmented version of the likelihoods, this is only valid for the VGP model at the moment.","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"The two next methods rely on numerical approximation of an integral and I therefore recommend using the VanillaGradDescent as it will use anyway the natural gradient updates. Adam seem to give random results.","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"QuadratureVI : Variational Inference with gradients computed by estimating the expected log-likelihood via quadrature.\nMCIntegrationVI : Variational Inference with gradients computed by estimating the expected log-likelihood via Monte Carlo Integration","category":"page"},{"location":"userguide/#compat_table-1","page":"User Guide","title":"Compatibility table","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"Not all inference are implemented/valid for all likelihoods, here is the compatibility table between them.","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"Likelihood/Inference AnalyticVI GibbsSampling QuadratureVI MCIntegrationVI\nGaussianLikelihood â â â â\nStudentTLikelihood â â â â\nLaplaceLikelihood â â â â\nHeteroscedasticLikelihood â (dev) (dev) â\nLogisticLikelihood â â â â\nBayesianSVM â (dev) â â\nLogisticSoftMaxLikelihood â â â (dev)\nSoftMaxLikelihood â â â (dev)\nPoisson â (dev) â â\nNegBinomialLikelihood â (dev) â â\n    ","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"(dev) means that the feature is possible and may be developped and tested but is not available yet. All contributions or requests are very welcome!","category":"page"},{"location":"userguide/#Additional-Parameters-1","page":"User Guide","title":"Additional Parameters","text":"","category":"section"},{"location":"userguide/#Hyperparameter-optimization-1","page":"User Guide","title":"Hyperparameter optimization","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"One can optimize the kernel hyperparameters as well as the inducing points location by maximizing the ELBO. All derivations are already hand-coded (no AD needed). One can select the optimization scheme via :","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"The optimizer keyword, can be nothing or false for no optimization or can be an optimizer from the GradDescent.jl package. By default it is set to Adam(Î±=0.01)\nThe Zoptimizer keyword, similar to optimizer it is used for optimizing the inducing points locations, it is by default set to nothing (no optimization)","category":"page"},{"location":"userguide/#meanprior-1","page":"User Guide","title":"PriorMean","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"The mean keyword allows you to add different types of prior means:","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"ZeroMean, a constant mean that cannot be optimized\nConstantMean, a constant mean that can be optimized\nEmpiricalMean, a vector mean with a different value for each point","category":"page"},{"location":"userguide/#IndependentPriors-1","page":"User Guide","title":"IndependentPriors","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"When having multiple latent Gaussian Processes one can decide to have a common prior for all of them or to have a separate prior for each latent GP. Having a common prior has the advantage that less computations are required to optimize hyperparameters.","category":"page"},{"location":"userguide/#train-1","page":"User Guide","title":"Training","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"Training is straightforward after initializing the model by running :","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"train!(model;iterations=100,callback=callbackfunction)","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"Where the callback option is for running a function at every iteration. callback function should be defined as","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"function callbackfunction(model,iter)\n    \"do things here\"...\nend","category":"page"},{"location":"userguide/#pred-1","page":"User Guide","title":"Prediction","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"Once the model has been trained it is finally possible to compute predictions. There always three possibilities :","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"predict_f(model,X_test,covf=true,fullcov=false) : Compute the parameters (mean and covariance) of the latent normal distributions of each test points. If covf=false return only the mean, if fullcov=true return a covariance matrix instead of only the diagonal\npredict_y(model,X_test) : Compute the point estimate of the predictive likelihood for regression or the label of the most likely class for classification.\nproba_y(model,X_test) : Return the mean with the variance of eahc point for regression or the predictive likelihood to obtain the class y=1 for classification.","category":"page"},{"location":"userguide/#Miscellaneous-1","page":"User Guide","title":"Miscellaneous","text":"","category":"section"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"ð§ In construction â Should be developed in the near future ð§","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"Saving/Loading models","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"Once a model has been trained it is possible to save its state in a file by using  save_trained_model(filename,model), a partial version of the file will be save in filename.","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"It is then possible to reload this file by using load_trained_model(filename). !!!However note that it will not be possible to train the model further!!! This function is only meant to do further predictions.","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"ð§ Pre-made callback functions ð§","category":"page"},{"location":"userguide/#","page":"User Guide","title":"User Guide","text":"There is one (for now) premade function to return a a MVHistory object and callback function for the training of binary classification problems. The callback will store the ELBO and the variational parameters at every iterations included in iterpoints If `Xtestandy_test` are provided it will also store the test accuracy and the mean and median test loglikelihood","category":"page"},{"location":"kernel/#Kernels-(Covariance-functions)-1","page":"Kernels","title":"Kernels (Covariance functions)","text":"","category":"section"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"The kernel function or covariance function is a crucial part of Gaussian Processes. It determines the covariance matrices between set of points, and its behaviour and parameters determines almost completely a GP behaviour.","category":"page"},{"location":"kernel/#Kernels-available-1","page":"Kernels","title":"Kernels available","text":"","category":"section"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"To get a short introduction of some covariance functions available one can look at these Slides from Rasmussen","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"We define theta_i as the lengthscale (in case of IsoKernel theta_i=thetaforall i) and sigma is the variance In this package covariance functions are progressively added for now the available kernels are :","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"RBF Kernel or Squared Exponential Kernel","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"k(xx) = sigma expleft(-frac12sum_i=1^Dfrac(x_i-x_i)^2theta_i^2right)","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"Matern Kernel","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"k(xx) = sigmafrac2^1-nuGamma(nu)(sqrt2nufracdrhoright)^nu K_nuleft(sqrt2nufracdrhoright)","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"More are coming, check the github projects for updates .","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"However the module for kernels should be replaced in the future by KernelFunctions.jl","category":"page"},{"location":"kernel/#Hyperparameter-optimization-1","page":"Kernels","title":"Hyperparameter optimization","text":"","category":"section"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"The advantage of Gaussian Processes is that it is possible to optimize all the hyperparameters of the model by optimizing the lower bound on the loglikelihood. One can compute the gradient of it and apply a classical gradient descent algorithm.","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"Unlike most other packages, the derivatives are all computed analytically. Since the hyperparameters intervene in gradients one needs to compute the matrix derivatives via the kernel derivatives. If K was defined via k(xx) then :","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"$","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"\\frac{d K}{d\\theta}  = J_\\theta$","category":"page"},{"location":"kernel/#","page":"Kernels","title":"Kernels","text":"Where J_theta was defined via fracdk(xx)dtheta, the rest of the work is simply matrix algebra.","category":"page"},{"location":"kernel/#!!!-In-construction-!!!-1","page":"Kernels","title":"!!! In construction !!!","text":"","category":"section"},{"location":"examples/#Examples-1","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#","page":"Examples","title":"Examples","text":"The best way to understand how the package is working is to look at examples. For each model you can find a Jupyter notebook on this repository","category":"page"},{"location":"comparison/#Julia-GP-Package-Comparison-1","page":"Julia GP Packages","title":"Julia GP Package Comparison","text":"","category":"section"},{"location":"comparison/#AugmentedGaussianProcesses.jl-vs-[Stheno.jl](https://github.com/willtebbutt/Stheno.jl)-vs-[GaussianProcesses.jl](https://github.com/STOR-i/GaussianProcesses.jl)-1","page":"Julia GP Packages","title":"AugmentedGaussianProcesses.jl vs Stheno.jl vs GaussianProcesses.jl","text":"","category":"section"},{"location":"comparison/#","page":"Julia GP Packages","title":"Julia GP Packages","text":"There are already two other Gaussian Process packages in Julia, however their feature are quite orthogonal. They are roughly compared here: AGP.jl stands for AugmentedGaussianProcesses.jl and GP.jl for GaussianProcesses.jl","category":"page"},{"location":"comparison/#Likelihood-1","page":"Julia GP Packages","title":"Likelihood","text":"","category":"section"},{"location":"comparison/#","page":"Julia GP Packages","title":"Julia GP Packages","text":"Likelihood AGP.jl Stheno.jl GP.jl\nGaussian â â (multi-input/multi-output) â\nStudent-T â â â\nBernoulli â (Logistic) â â (Probit)\nBayesian-SVM â â â\nPoisson â â â\nNegativeBinomial â â â\nExponential â â â\nMultiClass â â â","category":"page"},{"location":"comparison/#Inference-1","page":"Julia GP Packages","title":"Inference","text":"","category":"section"},{"location":"comparison/#","page":"Julia GP Packages","title":"Julia GP Packages","text":"Inference AGP.jl Stheno.jl GP.jl\nAnalytic (Gaussian only) â â â\nVariational Inference â (Analytic and Num. Appr.) â â\nGibbs-Sampling â â â\nMCMC â â â\nExpec. Propag. â â â","category":"page"},{"location":"comparison/#Kernels-1","page":"Julia GP Packages","title":"Kernels","text":"","category":"section"},{"location":"comparison/#","page":"Julia GP Packages","title":"Julia GP Packages","text":"Kernel AGP.jl Stheno.jl GP.jl\nRBF/Squared Exponential â â â\nMatern â â â\nConst â â â\nLinear â â â\nPoly â â â\nPeriodic â â â\nExponentiated Quadratic â â â\nRational Quadratic â â â\nWiener â â â\nSum of kernel â â â\nProduct of kernels â â â","category":"page"},{"location":"comparison/#","page":"Julia GP Packages","title":"Julia GP Packages","text":"Note that the kernels will be defered to MLKernels.jl in the future.","category":"page"},{"location":"comparison/#Other-features-1","page":"Julia GP Packages","title":"Other features","text":"","category":"section"},{"location":"comparison/#","page":"Julia GP Packages","title":"Julia GP Packages","text":"Feature AGP.jl Stheno.jl GP.jl\nSparse GP â â â\nCustom prior Mean â â â\nHyperparam. Opt. â ? â\nMultiOutput â â â","category":"page"},{"location":"api/#API-Library-1","page":"API","title":"API Library","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"","category":"page"},{"location":"api/#","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#","page":"API","title":"API","text":"CurrentModule = AugmentedGaussianProcesses","category":"page"},{"location":"api/#Module-1","page":"API","title":"Module","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"AugmentedGaussianProcesses","category":"page"},{"location":"api/#AugmentedGaussianProcesses.AugmentedGaussianProcesses","page":"API","title":"AugmentedGaussianProcesses.AugmentedGaussianProcesses","text":"General Framework for the data augmented Gaussian Processes\n\n\n\n\n\n","category":"module"},{"location":"api/#Model-Types-1","page":"API","title":"Model Types","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"GP\nVGP\nSVGP","category":"page"},{"location":"api/#AugmentedGaussianProcesses.GP","page":"API","title":"AugmentedGaussianProcesses.GP","text":"Class for Gaussian Processes models\n\nGP(X::AbstractArray{T}, y::AbstractArray, kernel::Kernel;\n    noise::Real=1e-5, opt_noise::Bool=true, verbose::Int=0,\n    optimizer::Bool=Adam(Î±=0.01),atfrequency::Int=1,\n    mean::Union{<:Real,AbstractVector{<:Real},PriorMean}=ZeroMean(),\n    IndependentPriors::Bool=true,ArrayType::UnionAll=Vector)\n\nArgument list :\n\nMandatory arguments\n\nX : input features, should be a matrix NÃD where N is the number of observation and D the number of dimension\ny : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)\nkernel : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models\n\nKeyword arguments\n\nnoise : Initial noise of the model\nopt_noise : Flag for optimizing the noise Ï=Î£(y-f)^2/N\nmean : Option for putting a prior mean\nverbose : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)\noptimizer : Optimizer for kernel hyperparameters (to be selected from GradDescent.jl) or set it to false to keep hyperparameters fixed\nIndependentPriors : Flag for setting independent or shared parameters among latent GPs\natfrequency : Choose how many variational parameters iterations are between hyperparameters optimization\nmean : PriorMean object, check the documentation on it MeanPrior\nArrayType : Option for using different type of array for storage (allow for GPU usage)\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.VGP","page":"API","title":"AugmentedGaussianProcesses.VGP","text":"Class for variational Gaussian Processes models (non-sparse)\n\nVGP(X::AbstractArray{T},y::AbstractVector,\nkernel::Kernel,\n    likelihood::LikelihoodType,inference::InferenceType;\n    verbose::Int=0,optimizer::Union{Bool,Optimizer,Nothing}=Adam(Î±=0.01),atfrequency::Integer=1,\n    mean::Union{<:Real,AbstractVector{<:Real},PriorMean}=ZeroMean(),\n    IndependentPriors::Bool=true,ArrayType::UnionAll=Vector)\n\nArgument list :\n\nMandatory arguments\n\nX : input features, should be a matrix NÃD where N is the number of observation and D the number of dimension\ny : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)\nkernel : covariance function, a single kernel from the KernelFunctions.jl package\nlikelihood : likelihood of the model, currently implemented : Gaussian, Bernoulli (with logistic link), Multiclass (softmax or logistic-softmax) see Likelihood Types\ninference : inference for the model, can be analytic, numerical or by sampling, check the model documentation to know what is available for your likelihood see the Compatibility Table\n\nKeyword arguments\n\nverbose : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)\noptimizer : Optimizer for kernel hyperparameters (to be selected from GradDescent.jl) or set it to false to keep hyperparameters fixed\natfrequency : Choose how many variational parameters iterations are between hyperparameters optimization\nmean : PriorMean object, check the documentation on it MeanPrior\nIndependentPriors : Flag for setting independent or shared parameters among latent GPs\nArrayType : Option for using different type of array for storage (allow for GPU usage)\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.SVGP","page":"API","title":"AugmentedGaussianProcesses.SVGP","text":"Class for sparse variational Gaussian Processes\n\nSVGP(X::AbstractArray{T1},y::AbstractVector{T2},kernel::Kernel,\n    likelihood::LikelihoodType,inference::InferenceType, nInducingPoints::Int;\n    verbose::Int=0,optimizer::Union{Optimizer,Nothing,Bool}=Adam(Î±=0.01),atfrequency::Int=1,\n    mean::Union{<:Real,AbstractVector{<:Real},PriorMean}=ZeroMean(),\n    Zoptimizer::Union{Optimizer,Nothing,Bool}=false,\n    ArrayType::UnionAll=Vector)\n\nArgument list :\n\nMandatory arguments\n\nX : input features, should be a matrix NÃD where N is the number of observation and D the number of dimension\ny : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)\nkernel : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models\nlikelihood : likelihood of the model, currently implemented : Gaussian, Student-T, Laplace, Bernoulli (with logistic link), Bayesian SVM, Multiclass (softmax or logistic-softmax) see Likelihood\ninference : inference for the model, can be analytic, numerical or by sampling, check the model documentation to know what is available for your likelihood see the Compatibility table\nnInducingPoints : number of inducing points\n\nOptional arguments\n\nverbose : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)\noptimizer : Optimizer for kernel hyperparameters (to be selected from GradDescent.jl) or set it to false to keep hyperparameters fixed\natfrequency : Choose how many variational parameters iterations are between hyperparameters optimization\nmean : PriorMean object, check the documentation on it MeanPrior\nIndependentPriors : Flag for setting independent or shared parameters among latent GPs\noptimizer : Optimizer for inducing point locations (to be selected from GradDescent.jl)\nArrayType : Option for using different type of array for storage (allow for GPU usage)\n\n\n\n\n\n","category":"type"},{"location":"api/#Likelihood-Types-1","page":"API","title":"Likelihood Types","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"GaussianLikelihood\nStudentTLikelihood\nLaplaceLikelihood\nLogisticLikelihood\nHeteroscedasticLikelihood\nBayesianSVM\nSoftMaxLikelihood\nLogisticSoftMaxLikelihood\nPoissonLikelihood\nNegBinomialLikelihood","category":"page"},{"location":"api/#AugmentedGaussianProcesses.GaussianLikelihood","page":"API","title":"AugmentedGaussianProcesses.GaussianLikelihood","text":"GaussianLikelihood(ÏÂ²::T=1e-3) #ÏÂ² is the variance\n\nGaussian noise :\n\n    p(yf) = N(yfÏÂ²)\n\nThere is no augmentation needed for this likelihood which is already conjugate to a Gaussian prior\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.StudentTLikelihood","page":"API","title":"AugmentedGaussianProcesses.StudentTLikelihood","text":"StudentTLikelihood(Î½::T,Ï::Real=one(T))\n\nStudent-t likelihood for regression:\n\n    p(yfÎ½Ï) = Î(05(Î½+1))(sqrt(Î½Ï) Ï Î(05Î½)) * (1+(y-f)^2(Ï^2Î½))^(-05(Î½+1))\n\nÎ½ is the number of degrees of freedom and Ï is the variance for local scale of the data.\n\n\n\nFor the analytical solution, it is augmented via:\n\n    p(yfÏ) = N(yfÏ^2 Ï)\n\nWhere Ï ~ IG(0.5Î½,,0.5Î½) where IG is the inverse gamma distribution See paper Robust Gaussian Process Regression with a Student-t Likelihood\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.LaplaceLikelihood","page":"API","title":"AugmentedGaussianProcesses.LaplaceLikelihood","text":"LaplaceLikelihood(Î²::T=1.0)  #  Laplace likelihood with scale Î²\n\nLaplace likelihood for regression:\n\n1(2Î²) exp(-y-fÎ²)\n\nsee wiki page\n\nFor the analytical solution, it is augmented via:\n\np(yfÏ) = N(yfÏÂ¹)\n\nwhere Ï  Exp(Ï  1(2 Î²^2)), and Exp is the Exponential distribution We use the variational distribution q(Ï) = GIG(Ï  abp)\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.LogisticLikelihood","page":"API","title":"AugmentedGaussianProcesses.LogisticLikelihood","text":"LogisticLikelihood()\n\nBernoulli likelihood with a logistic link for the Bernoulli likelihood\n\n    p(yf) = sigma(yf) = frac11+exp(-yf)\n\n(for more info see : wiki page)\n\n\n\nFor the analytic version the likelihood, it is augmented via:\n\n    p(yfÏ) = exp(05(yf - (yf)^2 Ï))\n\nwhere Ï  PG(Ï  1 0), and PG is the Polya-Gamma distribution See paper : Efficient Gaussian Process Classification Using Polya-Gamma Data Augmentation\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.HeteroscedasticLikelihood","page":"API","title":"AugmentedGaussianProcesses.HeteroscedasticLikelihood","text":"HeteroscedasticLikelihood(Î»::T=1.0)\n\nGaussian with heteroscedastic noise given by another gp:\n\n    p(yfg) = N(yf(Î» Ï(g))Â¹)\n\nWhere Ï is the logistic function\n\nAugmentation will be described in a future paper\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.BayesianSVM","page":"API","title":"AugmentedGaussianProcesses.BayesianSVM","text":"BayesianSVM()\n\nThe Bayesian SVM is a Bayesian interpretation of the classical SVM.\n\np(yf)  exp(2 max(1-yf0))\n\n\n---\nFor the analytic version of the likelihood it is augmented via\n\nmath p(y|f,Ï) = 1/(sqrt(2ÏÏ) exp(-0.5((1+Ï-yf)^2/Ï)) `whereÏ â¼ ð[0,â)`` has an improper prior (his posterior is however has a valid distribution, a Generalized Inverse Gaussian). For reference see this paper\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.SoftMaxLikelihood","page":"API","title":"AugmentedGaussianProcesses.SoftMaxLikelihood","text":"    SoftMaxLikelihood()\n\nMulticlass likelihood with Softmax transformation:\n\np(y=ifâ) = exp(fáµ¢) âexp(fâ)\n\nThere is no possible augmentation for this likelihood\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood","page":"API","title":"AugmentedGaussianProcesses.LogisticSoftMaxLikelihood","text":"    LogisticSoftMaxLikelihood()\n\nThe multiclass likelihood with a logistic-softmax mapping: :\n\np(y=ifââá´·) = Ï(fáµ¢)â Ï(fâ)\n\nwhere Ï is the logistic function. This likelihood has the same properties as softmax. â-\n\nFor the analytical version, the likelihood is augmented multiple times. More details can be found in the paper Multi-Class Gaussian Process Classification Made Conjugate: Efficient Inference via Data Augmentation\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.PoissonLikelihood","page":"API","title":"AugmentedGaussianProcesses.PoissonLikelihood","text":"    Poisson Likelihood(Î»::T=1.0)\n\nPoisson Likelihood where a Poisson distribution is defined at every point in space (careful, it's different from continous Poisson processes)\n\n    p(yf) = Poisson(yÎ»Ï(f))\n\nWhere Ï is the logistic function Augmentation details will be released at some point (open an issue if you want to see them)\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.NegBinomialLikelihood","page":"API","title":"AugmentedGaussianProcesses.NegBinomialLikelihood","text":"    NegBinomialLikelihood(r::Int=10)\n\nNegative Binomial likelihood with number of failures r\n\n    p(yrf) = binomial(y+r-1y) (1-Ï(f))Ê³Ï(f)Ê¸\n\nWhere Ï is the logistic function\n\n\n\n\n\n","category":"type"},{"location":"api/#Inference-Types-1","page":"API","title":"Inference Types","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"AnalyticVI\nAnalyticSVI\nGibbsSampling\nQuadratureVI\nQuadratureSVI\nMCIntegrationVI\nMCIntegrationSVI","category":"page"},{"location":"api/#AugmentedGaussianProcesses.AnalyticVI","page":"API","title":"AugmentedGaussianProcesses.AnalyticVI","text":"AnalyticVI\n\nVariational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation) All data is used at each iteration (use AnalyticSVI for Stochastic updates)\n\nAnalyticVI(;Ïµ::T=1e-5)\n\nKeywords arguments\n\n- `Ïµ::T` : convergence criteria\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.AnalyticSVI","page":"API","title":"AugmentedGaussianProcesses.AnalyticSVI","text":"AnalyticSVI Stochastic Variational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation)\n\nAnalyticSVI(nMinibatch::Integer;Ïµ::T=1e-5,optimizer::Optimizer=InverseDecay())\n\n- `nMinibatch::Integer` : Number of samples per mini-batches\n\nKeywords arguments\n\n- `Ïµ::T` : convergence criteria\n- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl](https://github.com/jacobcvt12/GradDescent.jl) package. Default is `InverseDecay()` (Ï=(Ï+iter)^-Îº)\n\n\n\n\n\n","category":"function"},{"location":"api/#AugmentedGaussianProcesses.GibbsSampling","page":"API","title":"AugmentedGaussianProcesses.GibbsSampling","text":"GibbsSampling(;Ïµ::T=1e-5,nBurnin::Int=100,samplefrequency::Int=1)\n\nDraw samples from the true posterior via Gibbs Sampling.\n\nKeywords arguments     - Ïµ::T : convergence criteria     - nBurnin::Int : Number of samples discarded before starting to save samples     - samplefrequency::Int : Frequency of sampling\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.QuadratureVI","page":"API","title":"AugmentedGaussianProcesses.QuadratureVI","text":"QuadratureVI\n\nVariational Inference solver by approximating gradients via numerical integration via Quadrature\n\nQuadratureVI(Ïµ::T=1e-5,nGaussHermite::Integer=20,optimizer::Optimizer=Momentum(Î·=0.0001))\n\nKeyword arguments\n\n- `Ïµ::T` : convergence criteria\n- `nGaussHermite::Int` : Number of points for the integral estimation\n- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl](https://github.com/jacobcvt12/GradDescent.jl) package. Default is `Momentum(Î·=0.0001)`\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.QuadratureSVI","page":"API","title":"AugmentedGaussianProcesses.QuadratureSVI","text":"QuadratureSVI\n\nStochastic Variational Inference solver by approximating gradients via numerical integration via Quadrature\n\nQuadratureSVI(nMinibatch::Integer;Ïµ::T=1e-5,nGaussHermite::Integer=20,optimizer::Optimizer=Adam(Î±=0.1))\n\n-`nMinibatch::Integer` : Number of samples per mini-batches\n\nKeyword arguments\n\n- `Ïµ::T` : convergence criteria, which can be user defined\n- `nGaussHermite::Int` : Number of points for the integral estimation (for the QuadratureVI)\n- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl](https://github.com/jacobcvt12/GradDescent.jl) package. Default is `Momentum(Î·=0.001)`\n\n\n\n\n\n","category":"function"},{"location":"api/#AugmentedGaussianProcesses.MCIntegrationVI","page":"API","title":"AugmentedGaussianProcesses.MCIntegrationVI","text":"MCIntegrationVI(;Ïµ::T=1e-5,nMC::Integer=1000,optimizer::Optimizer=Adam(Î±=0.1))\n\nVariational Inference solver by approximating gradients via MC Integration.\n\nKeyword arguments\n\n- `Ïµ::T` : convergence criteria, which can be user defined\n- `nMC::Int` : Number of samples per data point for the integral evaluation\n- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl]() package. Default is `Adam()`\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.MCIntegrationSVI","page":"API","title":"AugmentedGaussianProcesses.MCIntegrationSVI","text":"MCIntegrationSVI(;Ïµ::T=1e-5,nMC::Integer=1000,optimizer::Optimizer=Adam(Î±=0.1))\n\nStochastic Variational Inference solver by approximating gradients via Monte Carlo integration\n\nArgument\n\n-`nMinibatch::Integer` : Number of samples per mini-batches\n\nKeyword arguments\n\n- `Ïµ::T` : convergence criteria, which can be user defined\n- `nMC::Int` : Number of samples per data point for the integral evaluation\n- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl]() package. Default is `Adam()`\n\n\n\n\n\n","category":"function"},{"location":"api/#Functions-and-methods-1","page":"API","title":"Functions and methods","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"train!\npredict_f\npredict_y\nproba_y","category":"page"},{"location":"api/#AugmentedGaussianProcesses.train!","page":"API","title":"AugmentedGaussianProcesses.train!","text":"train!(model::AbstractGP;iterations::Integer=100,callback=0,convergence=0)\n\nFunction to train the given GP model.\n\nKeyword Arguments\n\nthere are options to change the number of max iterations,\n\niterations::Int : Number of iterations (not necessarily epochs!)for training\ncallback::Function : Callback function called at every iteration. Should be of type function(model,iter) ...  end\nconvergence::Function : Convergence function to be called every iteration, should return a scalar and take the same arguments as callback\n\n\n\n\n\n","category":"function"},{"location":"api/#AugmentedGaussianProcesses.predict_y","page":"API","title":"AugmentedGaussianProcesses.predict_y","text":"predict_y(model::AbstractGP,X_test::AbstractMatrix)\n\nReturn     - the predictive mean of X_test for regression     - the sign of X_test for classification     - the most likely class for multi-class classification     - the expected number of events for an event likelihood\n\n\n\n\n\n","category":"function"},{"location":"api/#AugmentedGaussianProcesses.proba_y","page":"API","title":"AugmentedGaussianProcesses.proba_y","text":"proba_y(model::AbstractGP,X_test::AbstractMatrix)\n\nReturn the probability distribution p(ytest|model,Xtest) :\n\n- Tuple of vectors of mean and variance for regression\n- Vector of probabilities of y_test = 1 for binary classification\n- Dataframe with columns and probability per class for multi-class classification\n\n\n\n\n\n","category":"function"},{"location":"api/#Kernels-1","page":"API","title":"Kernels","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"RBFKernel\nMaternKernel","category":"page"},{"location":"api/#Kernel-functions-1","page":"API","title":"Kernel functions","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"kernelmatrix\nkernelmatrix!\ngetvariance\ngetlengthscales","category":"page"},{"location":"api/#Prior-Means-1","page":"API","title":"Prior Means","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"ZeroMean\nConstantMean\nEmpiricalMean","category":"page"},{"location":"api/#AugmentedGaussianProcesses.ZeroMean","page":"API","title":"AugmentedGaussianProcesses.ZeroMean","text":"ZeroMean\n\nZeroMean()\n\nConstruct a mean prior set to 0 and cannot be changed.\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.ConstantMean","page":"API","title":"AugmentedGaussianProcesses.ConstantMean","text":"ConstantMean\n\nConstantMean(c::T=1.0;opt::Optimizer=Adam(Î±=0.01))\n\nConstruct a prior mean with constant c Optionally set an optimizer opt (Adam(Î±=0.01) by default)\n\n\n\n\n\n","category":"type"},{"location":"api/#AugmentedGaussianProcesses.EmpiricalMean","page":"API","title":"AugmentedGaussianProcesses.EmpiricalMean","text":"EmpiricalMean julia` function EmpiricalMean(c::V=1.0;opt::Optimizer=Adam(Î±=0.01)) where {V<:AbstractVector{<:Real}} Construct a constant mean with values c Optionally give an optimizer opt (Adam(Î±=0.01) by default)\n\n\n\n\n\n","category":"type"},{"location":"api/#Index-1","page":"API","title":"Index","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Pages = [\"api.md\"]\nModule = [\"AugmentedGaussianProcesses\"]\nOrder = [:type, :function]","category":"page"}]
}
