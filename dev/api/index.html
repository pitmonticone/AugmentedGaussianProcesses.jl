<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · AugmentedGaussianProcesses</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-129106538-2', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/icon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="AugmentedGaussianProcesses logo"/></a><div class="docs-package-name"><span class="docs-autofit">AugmentedGaussianProcesses</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../background/">Background</a></li><li><a class="tocitem" href="../userguide/">User Guide</a></li><li><a class="tocitem" href="../kernel/">Kernels</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/gpregression/">GP Regression</a></li><li><a class="tocitem" href="../examples/gpclassification/">GP Classification</a></li><li><a class="tocitem" href="../examples/multiclassgp/">Multi-Class GP</a></li><li><a class="tocitem" href="../examples/onlinegp/">Online GP</a></li><li><a class="tocitem" href="../examples/gpevents/">GP with event data</a></li></ul></li><li><a class="tocitem" href="../comparison/">Julia GP Packages</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Module"><span>Module</span></a></li><li><a class="tocitem" href="#Model-Types"><span>Model Types</span></a></li><li><a class="tocitem" href="#Likelihood-Types"><span>Likelihood Types</span></a></li><li><a class="tocitem" href="#Inference-Types"><span>Inference Types</span></a></li><li><a class="tocitem" href="#Functions-and-methods"><span>Functions and methods</span></a></li><li><a class="tocitem" href="#Prior-Means"><span>Prior Means</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Library"><a class="docs-heading-anchor" href="#API-Library">API Library</a><a id="API-Library-1"></a><a class="docs-heading-anchor-permalink" href="#API-Library" title="Permalink"></a></h1><hr/><ul><li><a href="#API-Library">API Library</a></li><ul><li><a href="#Module">Module</a></li><li><a href="#Model-Types">Model Types</a></li><li><a href="#Likelihood-Types">Likelihood Types</a></li><li><a href="#Inference-Types">Inference Types</a></li><li><a href="#Functions-and-methods">Functions and methods</a></li><li><a href="#Prior-Means">Prior Means</a></li><li><a href="#Index">Index</a></li></ul></ul><h2 id="Module"><a class="docs-heading-anchor" href="#Module">Module</a><a id="Module-1"></a><a class="docs-heading-anchor-permalink" href="#Module" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.AugmentedGaussianProcesses" href="#AugmentedGaussianProcesses.AugmentedGaussianProcesses"><code>AugmentedGaussianProcesses.AugmentedGaussianProcesses</code></a> — <span class="docstring-category">Module</span></header><section><div><p>General Framework for the data augmented Gaussian Processes</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/AugmentedGaussianProcesses.jl#L1-L5">source</a></section></article><h2 id="Model-Types"><a class="docs-heading-anchor" href="#Model-Types">Model Types</a><a id="Model-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.GP" href="#AugmentedGaussianProcesses.GP"><code>AugmentedGaussianProcesses.GP</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Class for Gaussian Processes models</p><pre><code class="language-julia">GP(X::AbstractArray{T}, y::AbstractArray, kernel::Kernel;
    noise::Real=1e-5, opt_noise::Bool=true, verbose::Int=0,
    optimiser=ADAM(0.01),atfrequency::Int=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    IndependentPriors::Bool=true,ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N×D where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>noise</code> : Initial noise of the model</li><li><code>opt_noise</code> : Flag for optimizing the noise σ=Σ(y-f)^2/N</li><li><code>mean</code> : Option for putting a prior mean</li><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/models/GP.jl#L1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.VGP" href="#AugmentedGaussianProcesses.VGP"><code>AugmentedGaussianProcesses.VGP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">VGP(X::AbstractArray{T},y::AbstractVector,
    kernel::Kernel,
    likelihood::LikelihoodType,inference::InferenceType;
    verbose::Int=0,optimiser=ADAM(0.01),atfrequency::Integer=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    IndependentPriors::Bool=true,ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N×D where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, a single kernel from the KernelFunctions.jl package</li><li><code>likelihood</code> : likelihood of the model, currently implemented : Gaussian, Bernoulli (with logistic link), Multiclass (softmax or logistic-softmax) see <a href="../userguide/#likelihood_user"><code>Likelihood Types</code></a></li><li><code>inference</code> : inference for the model, can be analytic, numerical or by sampling, check the model documentation to know what is available for your likelihood see the <a href="../userguide/#compat_table"><code>Compatibility Table</code></a></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/models/VGP.jl#L1-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.MCGP" href="#AugmentedGaussianProcesses.MCGP"><code>AugmentedGaussianProcesses.MCGP</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Class for variational Gaussian Processes models (non-sparse)</p><pre><code class="language-julia">MCGP(X::AbstractArray{T1,N1},y::AbstractArray{T2,N2},kernel::Union{Kernel,AbstractVector{&lt;:Kernel}},
    likelihood::LikelihoodType,inference::InferenceType;
    verbose::Int=0,optimiser=ADAM(0.01),atfrequency::Integer=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    IndependentPriors::Bool=true,ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N×D where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li><li><code>likelihood</code> : likelihood of the model, currently implemented : Gaussian, Bernoulli (with logistic link), Multiclass (softmax or logistic-softmax) see <a href="../userguide/#likelihood_user"><code>Likelihood Types</code></a></li><li><code>inference</code> : inference for the model, can be analytic, numerical or by sampling, check the model documentation to know what is available for your likelihood see the <a href="../userguide/#compat_table"><code>Compatibility Table</code></a></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/models/MCGP.jl#L1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.SVGP" href="#AugmentedGaussianProcesses.SVGP"><code>AugmentedGaussianProcesses.SVGP</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Class for sparse variational Gaussian Processes</p><pre><code class="language-none">SVGP(X::AbstractArray{T1},y::AbstractVector{T2},kernel::Kernel,
    likelihood::LikelihoodType,inference::InferenceType, nInducingPoints::Int;
    verbose::Int=0,optimiser=ADAM(0.001),atfrequency::Int=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    Zoptimiser=false,
    ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N×D where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li><li><code>likelihood</code> : likelihood of the model, currently implemented : Gaussian, Student-T, Laplace, Bernoulli (with logistic link), Bayesian SVM, Multiclass (softmax or logistic-softmax) see <a href="../userguide/#likelihood_user"><code>Likelihood</code></a></li><li><code>inference</code> : inference for the model, can be analytic, numerical or by sampling, check the model documentation to know what is available for your likelihood see the <a href="../userguide/#compat_table"><code>Compatibility table</code></a></li><li><code>nInducingPoints</code> : number of inducing points</li></ul><p><strong>Optional arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimiser</code> : Optimiser used for the kernel parameters. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior"><code>MeanPrior</code></a></li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>Zoptimiser</code> : Optimiser used for the inducing points locations. Should be an Optimiser object from the <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a> library, see list here <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">Optimisers</a> and on <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl">this list</a>. Default is <code>ADAM(0.001)</code></li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/models/SVGP.jl#L1-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.OnlineSVGP" href="#AugmentedGaussianProcesses.OnlineSVGP"><code>AugmentedGaussianProcesses.OnlineSVGP</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Class for sparse variational Gaussian Processes </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/models/OnlineSVGP.jl#L1">source</a></section></article><h2 id="Likelihood-Types"><a class="docs-heading-anchor" href="#Likelihood-Types">Likelihood Types</a><a id="Likelihood-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Likelihood-Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.GaussianLikelihood" href="#AugmentedGaussianProcesses.GaussianLikelihood"><code>AugmentedGaussianProcesses.GaussianLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">GaussianLikelihood(σ²::T=1e-3) #σ² is the variance</code></pre><p>Gaussian noise :</p><p class="math-container">\[    p(y|f) = N(y|f,σ²)\]</p><p>There is no augmentation needed for this likelihood which is already conjugate to a Gaussian prior</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/gaussian.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.StudentTLikelihood" href="#AugmentedGaussianProcesses.StudentTLikelihood"><code>AugmentedGaussianProcesses.StudentTLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StudentTLikelihood(ν::T,σ::Real=one(T))</code></pre><p><a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student-t likelihood</a> for regression:</p><p class="math-container">\[    p(y|f,ν,σ) = Γ(0.5(ν+1))/(sqrt(νπ) σ Γ(0.5ν)) * (1+(y-f)^2/(σ^2ν))^(-0.5(ν+1))\]</p><p><code>ν</code> is the number of degrees of freedom and <code>σ</code> is the variance for local scale of the data.</p><hr/><p>For the analytical solution, it is augmented via:</p><p class="math-container">\[    p(y|f,ω) = N(y|f,σ^2 ω)\]</p><p>Where <code>ω ~ IG(0.5ν,,0.5ν)</code> where <code>IG</code> is the inverse gamma distribution See paper <a href="http://www.jmlr.org/papers/volume12/jylanki11a/jylanki11a.pdf">Robust Gaussian Process Regression with a Student-t Likelihood</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/studentt.jl#L1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.LaplaceLikelihood" href="#AugmentedGaussianProcesses.LaplaceLikelihood"><code>AugmentedGaussianProcesses.LaplaceLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LaplaceLikelihood(β::T=1.0)  #  Laplace likelihood with scale β</code></pre><p>Laplace likelihood for regression:</p><p class="math-container">\[1/(2β) exp(-|y-f|/β)\]</p><p><strong>see <a href="https://en.wikipedia.org/wiki/Laplace_distribution">wiki page</a></strong></p><p>For the analytical solution, it is augmented via:</p><p class="math-container">\[p(y|f,ω) = N(y|f,ω⁻¹)\]</p><p>where <span>$ω ~ Exp(ω | 1/(2 β^2))$</span>, and <code>Exp</code> is the <a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential distribution</a> We use the variational distribution <span>$q(ω) = GIG(ω | a,b,p)$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/laplace.jl#L1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.LogisticLikelihood" href="#AugmentedGaussianProcesses.LogisticLikelihood"><code>AugmentedGaussianProcesses.LogisticLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LogisticLikelihood()</code></pre><p>Bernoulli likelihood with a logistic link for the Bernoulli likelihood</p><p class="math-container">\[    p(y|f) = \sigma(yf) = \frac{1}{1+\exp(-yf)},\]</p><p>(for more info see : <a href="https://en.wikipedia.org/wiki/Logistic_function">wiki page</a>)</p><hr/><p>For the analytic version the likelihood, it is augmented via:</p><p class="math-container">\[    p(y|f,ω) = exp(0.5(yf - (yf)^2 ω))\]</p><p>where <span>$ω ~ PG(ω | 1, 0)$</span>, and <code>PG</code> is the Polya-Gamma distribution See paper : <a href="https://arxiv.org/abs/1802.06383">Efficient Gaussian Process Classification Using Polya-Gamma Data Augmentation</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/logistic.jl#L1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.HeteroscedasticLikelihood" href="#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>AugmentedGaussianProcesses.HeteroscedasticLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">HeteroscedasticLikelihood(λ::T=1.0)</code></pre><p>Gaussian with heteroscedastic noise given by another gp:</p><p class="math-container">\[    p(y|f,g) = N(y|f,(λ σ(g))⁻¹)\]</p><p>Where <code>σ</code> is the logistic function</p><p>Augmentation will be described in a future paper</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/heteroscedastic.jl#L1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.BayesianSVM" href="#AugmentedGaussianProcesses.BayesianSVM"><code>AugmentedGaussianProcesses.BayesianSVM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BayesianSVM()</code></pre><p>The <a href="https://arxiv.org/abs/1707.05532">Bayesian SVM</a> is a Bayesian interpretation of the classical SVM.</p><p class="math-container">\[p(y|f) \propto \exp(2 \max(1-yf, 0))
````

---

For the analytic version of the likelihood, it is augmented via:
\]</p><p>math p(y|f, ω) = \frac{1}{\sqrt(2\pi\omega) \exp(-\frac{(1+\omega-yf)^2}{2\omega})) ```</p><p>where <span>$ω ∼ 𝟙[0,∞)$</span> has an improper prior (his posterior is however has a valid distribution, a Generalized Inverse Gaussian). For reference <a href="http://ecmlpkdd2017.ijs.si/papers/paperID502.pdf">see this paper</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/bayesiansvm.jl#L1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.SoftMaxLikelihood" href="#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>AugmentedGaussianProcesses.SoftMaxLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">    SoftMaxLikelihood()</code></pre><p>Multiclass likelihood with <a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax transformation</a>:</p><p class="math-container">\[p(y=i|{fₖ}) = exp(fᵢ)/ ∑ₖexp(fₖ)\]</p><p>There is no possible augmentation for this likelihood</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/softmax.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.LogisticSoftMaxLikelihood" href="#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>AugmentedGaussianProcesses.LogisticSoftMaxLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LogisticSoftMaxLikelihood(num_class)</code></pre><p>The multiclass likelihood with a logistic-softmax mapping: :</p><p class="math-container">\[p(y=i|{fₖ}₁ᴷ) = σ(fᵢ)/∑ₖ σ(fₖ)\]</p><p>where <code>σ</code> is the logistic function. This likelihood has the same properties as <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>. –-</p><p>For the analytical version, the likelihood is augmented multiple times. More details can be found in the paper <a href="https://arxiv.org/abs/1905.09670">Multi-Class Gaussian Process Classification Made Conjugate: Efficient Inference via Data Augmentation</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/logisticsoftmax.jl#L1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.PoissonLikelihood" href="#AugmentedGaussianProcesses.PoissonLikelihood"><code>AugmentedGaussianProcesses.PoissonLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Poisson Likelihood(λ=1.0)</code></pre><p><a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson Likelihood</a> where a Poisson distribution is defined at every point in space (careful, it&#39;s different from continous Poisson processes)</p><p class="math-container">\[    p(y|f) = Poisson(y|\lambda \sigma(f))\]</p><p>Where <code>σ</code> is the logistic function Augmentation details will be released at some point (open an issue if you want to see them)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/poisson.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.NegBinomialLikelihood" href="#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>AugmentedGaussianProcesses.NegBinomialLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NegBinomialLikelihood(r::Real=10)</code></pre><p><a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">Negative Binomial likelihood</a> with number of failures <code>r</code></p><p class="math-container">\[    p(y|r, f) = binomial(y + r - 1, y) (1 - σ(f))ʳ σ(f)ʸ
    p(y|r, f) = Γ(y + r)/Γ(y + 1)Γ(r) (1 - σ(f))ʳ σ(f)ʸ\]</p><p>Where <code>σ</code> is the logistic function</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/likelihood/negativebinomial.jl#L1-L11">source</a></section></article><h2 id="Inference-Types"><a class="docs-heading-anchor" href="#Inference-Types">Inference Types</a><a id="Inference-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.AnalyticVI" href="#AugmentedGaussianProcesses.AnalyticVI"><code>AugmentedGaussianProcesses.AnalyticVI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AnalyticVI(;ϵ::T=1e-5)</code></pre><p>Variational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation) All data is used at each iteration (use AnalyticSVI for Stochastic updates)</p><p><strong>Keywords arguments</strong>     - <code>ϵ::T</code> : convergence criteria</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/inference/analyticVI.jl#L59-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.AnalyticSVI" href="#AugmentedGaussianProcesses.AnalyticSVI"><code>AugmentedGaussianProcesses.AnalyticSVI</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">AnalyticSVI(nMinibatch::Integer; ϵ::T=1e-5, optimiser=RobbinsMonro())</code></pre><p>Stochastic Variational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation)</p><ul><li><code>nMinibatch::Integer</code> : Number of samples per mini-batches</li></ul><p><strong>Keywords arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria
- `optimiser` : Optimiser used for the variational updates. Should be an Optimiser object from the [Flux.jl](https://github.com/FluxML/Flux.jl) library, see list here [Optimisers](https://fluxml.ai/Flux.jl/stable/training/optimisers/) and on [this list](https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl). Default is `RobbinsMonro()` (ρ=(τ+iter)^-κ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/inference/analyticVI.jl#L70-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.GibbsSampling" href="#AugmentedGaussianProcesses.GibbsSampling"><code>AugmentedGaussianProcesses.GibbsSampling</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">GibbsSampling(;ϵ::T=1e-5,nBurnin::Int=100,samplefrequency::Int=1)</code></pre><p>Draw samples from the true posterior via Gibbs Sampling.</p><p><strong>Keywords arguments</strong>     - <code>ϵ::T</code> : convergence criteria     - <code>nBurnin::Int</code> : Number of samples discarded before starting to save samples     - <code>samplefrequency::Int</code> : Frequency of sampling</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/inference/gibbssampling.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.QuadratureVI" href="#AugmentedGaussianProcesses.QuadratureVI"><code>AugmentedGaussianProcesses.QuadratureVI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">QuadratureVI(ϵ::T=1e-5,nGaussHermite::Integer=20,optimiser=Momentum(0.0001))</code></pre><p>Variational Inference solver by approximating gradients via numerical integration via Quadrature</p><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria
- `nGaussHermite::Int` : Number of points for the integral estimation
- `natural::Bool` : Use natural gradients
- `optimiser` : Optimiser used for the variational updates. Should be an Optimiser object from the [Flux.jl](https://github.com/FluxML/Flux.jl) library, see list here [Optimisers](https://fluxml.ai/Flux.jl/stable/training/optimisers/) and on [this list](https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl). Default is `Momentum(0.0001)`</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/inference/quadratureVI.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.QuadratureSVI" href="#AugmentedGaussianProcesses.QuadratureSVI"><code>AugmentedGaussianProcesses.QuadratureSVI</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">QuadratureSVI(nMinibatch::Integer;ϵ::T=1e-5,nGaussHermite::Integer=20,optimiser=Momentum(0.0001))</code></pre><p>Stochastic Variational Inference solver by approximating gradients via numerical integration via Quadrature</p><pre><code class="language-none">-`nMinibatch::Integer` : Number of samples per mini-batches</code></pre><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria, which can be user defined
- `nGaussHermite::Int` : Number of points for the integral estimation (for the QuadratureVI)
- `natural::Bool` : Use natural gradients
- `optimiser` : Optimiser used for the variational updates. Should be an Optimiser object from the [Flux.jl](https://github.com/FluxML/Flux.jl) library, see list here [Optimisers](https://fluxml.ai/Flux.jl/stable/training/optimisers/) and on [this list](https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl). Default is `Momentum(0.0001)`</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/inference/quadratureVI.jl#L106-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.MCIntegrationVI" href="#AugmentedGaussianProcesses.MCIntegrationVI"><code>AugmentedGaussianProcesses.MCIntegrationVI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MCIntegrationVI(;ϵ::T=1e-5,nMC::Integer=1000,optimiser=Momentum(0.001))</code></pre><p>Variational Inference solver by approximating gradients via MC Integration.</p><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria, which can be user defined
- `nMC::Int` : Number of samples per data point for the integral evaluation
- `natural::Bool` : Use natural gradients
- `optimiser` : Optimiser used for the variational updates. Should be an Optimiser object from the [Flux.jl](https://github.com/FluxML/Flux.jl) library, see list here [Optimisers](https://fluxml.ai/Flux.jl/stable/training/optimisers/) and on [this list](https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl). Default is `Momentum(0.01)`</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/inference/MCVI.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.MCIntegrationSVI" href="#AugmentedGaussianProcesses.MCIntegrationSVI"><code>AugmentedGaussianProcesses.MCIntegrationSVI</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">MCIntegrationSVI(;ϵ::T=1e-5,nMC::Integer=1000,optimiser=Momentum(0.0001))</code></pre><p>Stochastic Variational Inference solver by approximating gradients via Monte Carlo integration</p><p><strong>Argument</strong></p><pre><code class="language-none">-`nMinibatch::Integer` : Number of samples per mini-batches</code></pre><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria, which can be user defined
- `nMC::Int` : Number of samples per data point for the integral evaluation
- `natural::Bool` : Use natural gradients
- `optimiser` : Optimiser used for the variational updates. Should be an Optimiser object from the [Flux.jl](https://github.com/FluxML/Flux.jl) library, see list here [Optimisers](https://fluxml.ai/Flux.jl/stable/training/optimisers/) and on [this list](https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/src/inference/optimisers.jl). Default is `Momentum()` (ρ=(τ+iter)^-κ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/inference/MCVI.jl#L99-L114">source</a></section></article><h2 id="Functions-and-methods"><a class="docs-heading-anchor" href="#Functions-and-methods">Functions and methods</a><a id="Functions-and-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Functions-and-methods" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.train!" href="#AugmentedGaussianProcesses.train!"><code>AugmentedGaussianProcesses.train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">train!(model::AbstractGP;iterations::Integer=100,callback=0,convergence=0)</code></pre><p>Function to train the given GP <code>model</code>.</p><p><strong>Keyword Arguments</strong></p><p>there are options to change the number of max iterations,</p><ul><li><code>iterations::Int</code> : Number of iterations (not necessarily epochs!)for training</li><li><code>callback::Function</code> : Callback function called at every iteration. Should be of type <code>function(model,iter) ...  end</code></li><li><code>convergence::Function</code> : Convergence function to be called every iteration, should return a scalar and take the same arguments as <code>callback</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/training/training.jl#L1-L12">source</a></section><section><div><pre><code class="language-none">train!(model::AbstractGP, X::AbstractMatrix, y::AbstractVector;obsdim = 1, iterations::Int=10,callback=nothing,conv=0)
train!(model::AbstractGP, X::AbstractVector, y::AbstractVector;iterations::Int=20,callback=nothing,conv=0)</code></pre><p>Function to train the given GP <code>model</code>.</p><p><strong>Keyword Arguments</strong></p><p>there are options to change the number of max iterations,</p><ul><li><code>iterations::Int</code> : Number of iterations (not necessarily epochs!)for training</li><li><code>callback::Function</code> : Callback function called at every iteration. Should be of type <code>function(model,iter) ...  end</code></li><li><code>conv::Function</code> : Convergence function to be called every iteration, should return a scalar and take the same arguments as <code>callback</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/training/onlinetraining.jl#L1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.predict_f" href="#AugmentedGaussianProcesses.predict_f"><code>AugmentedGaussianProcesses.predict_f</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">predict_f(m::AbstractGP, X_test, cov::Bool=true, diag::Bool=true)</code></pre><p>Compute the mean of the predicted latent distribution of <code>f</code> on <code>X_test</code> for the variational GP <code>model</code></p><p>Return also the diagonal variance if <code>cov=true</code> and the full covariance if <code>diag=false</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/training/predictions.jl#L6-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.predict_y" href="#AugmentedGaussianProcesses.predict_y"><code>AugmentedGaussianProcesses.predict_y</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">predict_y(model::AbstractGP, X_test::AbstractVector)
predict_y(model::AbstractGP, X_test::AbstractMatrix; obsdim = 1)</code></pre><p>Return     - the predictive mean of <code>X_test</code> for regression     - the sign of <code>X_test</code> for classification     - the most likely class for multi-class classification     - the expected number of events for an event likelihood</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaLang/julia/blob/788b2c77c10c2160f4794a4d4b6b81a95a90940c/base/#L0-L9">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>proba_y</code>. Check Documenter&#39;s build log for details.</p></div></div><h2 id="Prior-Means"><a class="docs-heading-anchor" href="#Prior-Means">Prior Means</a><a id="Prior-Means-1"></a><a class="docs-heading-anchor-permalink" href="#Prior-Means" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.ZeroMean" href="#AugmentedGaussianProcesses.ZeroMean"><code>AugmentedGaussianProcesses.ZeroMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ZeroMean()</code></pre><p>Construct a mean prior set to <code>0</code> and which cannot be updated.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/mean/zeromean.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.ConstantMean" href="#AugmentedGaussianProcesses.ConstantMean"><code>AugmentedGaussianProcesses.ConstantMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ConstantMean(c::Real = 1.0;opt=ADAM(0.01))</code></pre><p>Construct a prior mean with constant <code>c</code> Optionally set an optimiser <code>opt</code> (<code>ADAM(0.01)</code> by default)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/mean/constantmean.jl#L6-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AugmentedGaussianProcesses.EmpiricalMean" href="#AugmentedGaussianProcesses.EmpiricalMean"><code>AugmentedGaussianProcesses.EmpiricalMean</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">EmpiricalMean(c::AbstractVector{&lt;:Real}=1.0;opt=ADAM(0.01))</code></pre><p>Construct a empirical mean with values <code>c</code> Optionally give an optimiser <code>opt</code> (<code>ADAM(0.01)</code> by default)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/bdf9ea65a23d07285b6f20416235f6dee7429ece/src/mean/empiricalmean.jl#L6-L11">source</a></section></article><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#AugmentedGaussianProcesses.AnalyticVI"><code>AugmentedGaussianProcesses.AnalyticVI</code></a></li><li><a href="#AugmentedGaussianProcesses.BayesianSVM"><code>AugmentedGaussianProcesses.BayesianSVM</code></a></li><li><a href="#AugmentedGaussianProcesses.ConstantMean"><code>AugmentedGaussianProcesses.ConstantMean</code></a></li><li><a href="#AugmentedGaussianProcesses.EmpiricalMean"><code>AugmentedGaussianProcesses.EmpiricalMean</code></a></li><li><a href="#AugmentedGaussianProcesses.GP"><code>AugmentedGaussianProcesses.GP</code></a></li><li><a href="#AugmentedGaussianProcesses.GaussianLikelihood"><code>AugmentedGaussianProcesses.GaussianLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.GibbsSampling"><code>AugmentedGaussianProcesses.GibbsSampling</code></a></li><li><a href="#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>AugmentedGaussianProcesses.HeteroscedasticLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LaplaceLikelihood"><code>AugmentedGaussianProcesses.LaplaceLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LogisticLikelihood"><code>AugmentedGaussianProcesses.LogisticLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>AugmentedGaussianProcesses.LogisticSoftMaxLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.MCGP"><code>AugmentedGaussianProcesses.MCGP</code></a></li><li><a href="#AugmentedGaussianProcesses.MCIntegrationVI"><code>AugmentedGaussianProcesses.MCIntegrationVI</code></a></li><li><a href="#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>AugmentedGaussianProcesses.NegBinomialLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.OnlineSVGP"><code>AugmentedGaussianProcesses.OnlineSVGP</code></a></li><li><a href="#AugmentedGaussianProcesses.PoissonLikelihood"><code>AugmentedGaussianProcesses.PoissonLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.QuadratureVI"><code>AugmentedGaussianProcesses.QuadratureVI</code></a></li><li><a href="#AugmentedGaussianProcesses.SVGP"><code>AugmentedGaussianProcesses.SVGP</code></a></li><li><a href="#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>AugmentedGaussianProcesses.SoftMaxLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.StudentTLikelihood"><code>AugmentedGaussianProcesses.StudentTLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.VGP"><code>AugmentedGaussianProcesses.VGP</code></a></li><li><a href="#AugmentedGaussianProcesses.ZeroMean"><code>AugmentedGaussianProcesses.ZeroMean</code></a></li><li><a href="#AugmentedGaussianProcesses.AnalyticSVI"><code>AugmentedGaussianProcesses.AnalyticSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.MCIntegrationSVI"><code>AugmentedGaussianProcesses.MCIntegrationSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.QuadratureSVI"><code>AugmentedGaussianProcesses.QuadratureSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.predict_f"><code>AugmentedGaussianProcesses.predict_f</code></a></li><li><a href="#AugmentedGaussianProcesses.predict_y"><code>AugmentedGaussianProcesses.predict_y</code></a></li><li><a href="#AugmentedGaussianProcesses.train!"><code>AugmentedGaussianProcesses.train!</code></a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../comparison/">« Julia GP Packages</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 13 January 2021 17:22">Wednesday 13 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
