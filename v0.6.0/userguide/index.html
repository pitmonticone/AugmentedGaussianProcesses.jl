<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>User Guide · AugmentedGaussianProcesses</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-129106538-2', 'auto');
ga('send', 'pageview');
</script><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/icon.ico" rel="icon" type="image/x-icon"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="AugmentedGaussianProcesses logo"/></a><h1>AugmentedGaussianProcesses</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../background/">Background</a></li><li class="current"><a class="toctext" href>User Guide</a><ul class="internal"><li><a class="toctext" href="#init-1">Initialization</a></li><li><a class="toctext" href="#train-1">Training</a></li><li><a class="toctext" href="#pred-1">Prediction</a></li><li><a class="toctext" href="#Miscellaneous-1">Miscellaneous</a></li></ul></li><li><a class="toctext" href="../kernel/">Kernels</a></li><li><a class="toctext" href="../examples/">Examples</a></li><li><a class="toctext" href="../comparison/">Julia GP Packages</a></li><li><a class="toctext" href="../api/">API</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>User Guide</a></li></ul><a class="edit-page" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/master/docs/src/userguide.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>User Guide</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="User-Guide-1" href="#User-Guide-1">User Guide</a></h1><p>There are 3 main actions needed to train and use the different models:</p><ul><li><a href="#init-1">Initialization</a></li><li><a href="#train-1">Training</a></li><li><a href="#pred-1">Prediction</a></li></ul><h2><a class="nav-anchor" id="init-1" href="#init-1">Initialization</a></h2><h3><a class="nav-anchor" id="GP-vs-VGP-vs-SVGP-1" href="#GP-vs-VGP-vs-SVGP-1">GP vs VGP vs SVGP</a></h3><p>There are currently 3 possible Gaussian Process models:</p><ul><li><a href="../api/#AugmentedGaussianProcesses.GP"><code>GP</code></a> corresponds to the original GP regression model, it is necessarily with a Gaussian likelihood.</li></ul><pre><code class="language-julia">    GP(X_train,y_train,kernel)</code></pre><ul><li><a href="../api/#AugmentedGaussianProcesses.VGP"><code>VGP</code></a> is a variational GP model: a multivariate Gaussian is approximating the true posterior. There is no inducing points augmentation involved. Therefore it is well suited for small datasets (~10^3 samples)</li></ul><pre><code class="language-julia">    VGP(X_train,y_train,kernel,likelihood,inference)</code></pre><ul><li><a href="../api/#AugmentedGaussianProcesses.SVGP"><code>SVGP</code></a> is a variational GP model augmented with inducing points. The optimization is done on those points, allowing for stochastic updates and large scalability. The counterpart can be a slightly lower accuracy and the need to select the number and the location of the inducing points (however this is a problem currently worked on).</li></ul><pre><code class="language-julia">    SVGP(X_train,y_train,kernel,likelihood,inference,n_inducingpoints)</code></pre><h3><a class="nav-anchor" id="likelihood_user-1" href="#likelihood_user-1">Likelihood</a></h3><p><code>GP</code> can only have a Gaussian likelihood, <code>VGP</code> and <code>SVGP</code> have more choices. Here are the ones currently implemented:</p><h4><a class="nav-anchor" id="Regression-1" href="#Regression-1">Regression</a></h4><p>For <strong>regression</strong>, four likelihoods are available :</p><ul><li>The classical <a href="../api/#AugmentedGaussianProcesses.GaussianLikelihood"><code>GaussianLikelihood</code></a>, for <a href="https://en.wikipedia.org/wiki/Gaussian_noise"><strong>Gaussian noise</strong></a></li><li>The <a href="../api/#AugmentedGaussianProcesses.StudentTLikelihood"><code>StudentTLikelihood</code></a>, assuming noise from a <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution"><strong>Student-T</strong></a> distribution (more robust to ouliers)</li><li>The <a href="../api/#AugmentedGaussianProcesses.LaplaceLikelihood"><code>LaplaceLikelihood</code></a>, with noise from a <a href="https://en.wikipedia.org/wiki/Laplace_distribution"><strong>Laplace</strong></a> distribution.</li><li>The <a href="../api/#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>HeteroscedasticLikelihood</code></a>, (in development) where the noise is a function of the input: <span>$\\text{Var}(X) = \\lambda\\sigma^{-1}(g(X))$</span> where <code>g(X)</code> is an additional Gaussian Process and <span>$\\sigma$</span> is the logistic function.</li></ul><h4><a class="nav-anchor" id="Classification-1" href="#Classification-1">Classification</a></h4><p>For <strong>classification</strong> one can select among</p><ul><li>The <a href="../api/#AugmentedGaussianProcesses.LogisticLikelihood"><code>LogisticLikelihood</code></a> : a Bernoulli likelihood with a <a href="https://en.wikipedia.org/wiki/Logistic_function"><strong>logistic link</strong></a></li><li>The <a href="../api/#AugmentedGaussianProcesses.BayesianSVM"><code>BayesianSVM</code></a> likelihood based on the <a href="https://en.wikipedia.org/wiki/Support_vector_machine#Bayesian_SVM"><strong>frequentist SVM</strong></a>, equivalent to use a hinge loss.</li></ul><h4><a class="nav-anchor" id="Event-Likelihoods-1" href="#Event-Likelihoods-1">Event Likelihoods</a></h4><p>For likelihoods such as Poisson or Negative Binomial, we approximate a parameter by <code>σ(f)</code>. Two Likelihoods are implemented :</p><ul><li>The <a href="../api/#AugmentedGaussianProcesses.PoissonLikelihood"><code>PoissonLikelihood</code></a> : A discrete <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson process</a> (one parameter per point) with the scale parameter defined as <code>λσ(f)</code></li><li>The <a href="../api/#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>NegBinomialLikelihood</code></a> : The <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">Negative Binomial likelihood</a> where <code>r</code> is fixed and we define the success probability <code>p</code> as <code>σ(f)</code></li></ul><h4><a class="nav-anchor" id="Multi-class-classification-1" href="#Multi-class-classification-1">Multi-class classification</a></h4><p>There is two available likelihoods for multi-class classification:</p><ul><li>The <a href="../api/#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>SoftMaxLikelihood</code></a>, the most common approach. However no analytical solving is possible</li><li>The <a href="../api/#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>LogisticSoftMaxLikelihood</code></a>, a modified softmax where the exponential function is replaced by the logistic function. It allows to get a fully conjugate model, <a href="https://arxiv.org/abs/1905.09670"><strong>Corresponding paper</strong></a></li></ul><h3><a class="nav-anchor" id="More-options-1" href="#More-options-1">More options</a></h3><p>You can also write your own likelihood by using the <a href="https://github.com/theogf/AugmentedGaussianProcesses.jl/tree/master/docs/src/template_likelihood.jl">following template</a>.</p><h3><a class="nav-anchor" id="Inference-1" href="#Inference-1">Inference</a></h3><p>Inference can be done in various ways.</p><ul><li><a href="../api/#AugmentedGaussianProcesses.AnalyticVI"><code>AnalyticVI</code></a> : <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Inference</a> with closed-form updates. For non-Gaussian likelihoods, this relies on augmented version of the likelihoods. For using Stochastic Variational Inference, one can use <a href="../api/#AugmentedGaussianProcesses.AnalyticSVI"><code>AnalyticSVI</code></a> with the size of the mini-batch as an argument</li><li><a href="../api/#AugmentedGaussianProcesses.GibbsSampling"><code>GibbsSampling</code></a> : Gibbs Sampling of the true posterior, this also rely on an augmented version of the likelihoods, this is only valid for the <code>VGP</code> model at the moment.</li></ul><p>The two next methods rely on numerical approximation of an integral and I therefore recommend using the <code>VanillaGradDescent</code> as it will use anyway the natural gradient updates. <code>Adam</code> seem to give random results.</p><ul><li><a href="../api/#AugmentedGaussianProcesses.QuadratureVI"><code>QuadratureVI</code></a> : Variational Inference with gradients computed by estimating the expected log-likelihood via quadrature.</li><li><a href="../api/#AugmentedGaussianProcesses.MCIntegrationVI"><code>MCIntegrationVI</code></a> : Variational Inference with gradients computed by estimating the expected log-likelihood via Monte Carlo Integration</li></ul><h3><a class="nav-anchor" id="compat_table-1" href="#compat_table-1">Compatibility table</a></h3><p>Not all inference are implemented/valid for all likelihoods, here is the compatibility table between them.</p><table><tr><th style="text-align: right">Likelihood/Inference</th><th style="text-align: center">AnalyticVI</th><th style="text-align: center">GibbsSampling</th><th style="text-align: center">QuadratureVI</th><th style="text-align: center">MCIntegrationVI</th></tr><tr><td style="text-align: right">GaussianLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">StudentTLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">LaplaceLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">HeteroscedasticLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">(dev)</td><td style="text-align: center">(dev)</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">LogisticLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">BayesianSVM</td><td style="text-align: center">✔</td><td style="text-align: center">(dev)</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">LogisticSoftMaxLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">✔</td><td style="text-align: center">✖</td><td style="text-align: center">(dev)</td></tr><tr><td style="text-align: right">SoftMaxLikelihood</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td><td style="text-align: center">(dev)</td></tr><tr><td style="text-align: right">Poisson</td><td style="text-align: center">✔</td><td style="text-align: center">(dev)</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">NegBinomialLikelihood</td><td style="text-align: center">✔</td><td style="text-align: center">(dev)</td><td style="text-align: center">✖</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right"></td><td style="text-align: center"></td><td style="text-align: center"></td><td style="text-align: center"></td><td style="text-align: center"></td></tr></table><p>(dev) means that the feature is possible and may be developped and tested but is not available yet. All contributions or requests are very welcome!</p><h3><a class="nav-anchor" id="Additional-Parameters-1" href="#Additional-Parameters-1">Additional Parameters</a></h3><h4><a class="nav-anchor" id="Hyperparameter-optimization-1" href="#Hyperparameter-optimization-1">Hyperparameter optimization</a></h4><p>One can optimize the kernel hyperparameters as well as the inducing points location by maximizing the ELBO. All derivations are already hand-coded (no AD needed). One can select the optimization scheme via :</p><ul><li>The <code>optimizer</code> keyword, can be <code>nothing</code> or <code>false</code> for no optimization or can be an optimizer from the <a href="https://github.com/jacobcvt12/GradDescent.jl">GradDescent.jl</a> package. By default it is set to <code>Adam(α=0.01)</code></li><li>The <code>Zoptimizer</code> keyword, similar to <code>optimizer</code> it is used for optimizing the inducing points locations, it is by default set to <code>nothing</code> (no optimization)</li></ul><h4><a class="nav-anchor" id="meanprior-1" href="#meanprior-1">PriorMean</a></h4><p>The <code>mean</code> keyword allows you to add different types of prior means:</p><ul><li><a href="../api/#AugmentedGaussianProcesses.ZeroMean"><code>ZeroMean</code></a>, a constant mean that cannot be optimized</li><li><a href="../api/#AugmentedGaussianProcesses.ConstantMean"><code>ConstantMean</code></a>, a constant mean that can be optimized</li><li><a href="../api/#AugmentedGaussianProcesses.EmpiricalMean"><code>EmpiricalMean</code></a>, a vector mean with a different value for each point</li></ul><h4><a class="nav-anchor" id="IndependentPriors-1" href="#IndependentPriors-1">IndependentPriors</a></h4><p>When having multiple latent Gaussian Processes one can decide to have a common prior for all of them or to have a separate prior for each latent GP. Having a common prior has the advantage that less computations are required to optimize hyperparameters.</p><h2><a class="nav-anchor" id="train-1" href="#train-1">Training</a></h2><p>Training is straightforward after initializing the <code>model</code> by running :</p><pre><code class="language-julia">train!(model;iterations=100,callback=callbackfunction)</code></pre><p>Where the <code>callback</code> option is for running a function at every iteration. <code>callback function should be defined as</code></p><pre><code class="language-julia">function callbackfunction(model,iter)
    &quot;do things here&quot;...
end</code></pre><h2><a class="nav-anchor" id="pred-1" href="#pred-1">Prediction</a></h2><p>Once the model has been trained it is finally possible to compute predictions. There always three possibilities :</p><ul><li><code>predict_f(model,X_test,covf=true,fullcov=false)</code> : Compute the parameters (mean and covariance) of the latent normal distributions of each test points. If <code>covf=false</code> return only the mean, if <code>fullcov=true</code> return a covariance matrix instead of only the diagonal</li><li><code>predict_y(model,X_test)</code> : Compute the point estimate of the predictive likelihood for regression or the label of the most likely class for classification.</li><li><code>proba_y(model,X_test)</code> : Return the mean with the variance of eahc point for regression or the predictive likelihood to obtain the class <code>y=1</code> for classification.</li></ul><h2><a class="nav-anchor" id="Miscellaneous-1" href="#Miscellaneous-1">Miscellaneous</a></h2><p>🚧 <strong>In construction – Should be developed in the near future</strong> 🚧</p><p>Saving/Loading models</p><p>Once a model has been trained it is possible to save its state in a file by using  <code>save_trained_model(filename,model)</code>, a partial version of the file will be save in <code>filename</code>.</p><p>It is then possible to reload this file by using <code>load_trained_model(filename)</code>. <strong>!!!However note that it will not be possible to train the model further!!!</strong> This function is only meant to do further predictions.</p><p>🚧 Pre-made callback functions 🚧</p><p>There is one (for now) premade function to return a a MVHistory object and callback function for the training of binary classification problems. The callback will store the ELBO and the variational parameters at every iterations included in iter<em>points If `X</em>test<code>and</code>y_test` are provided it will also store the test accuracy and the mean and median test loglikelihood</p><footer><hr/><a class="previous" href="../background/"><span class="direction">Previous</span><span class="title">Background</span></a><a class="next" href="../kernel/"><span class="direction">Next</span><span class="title">Kernels</span></a></footer></article></body></html>
